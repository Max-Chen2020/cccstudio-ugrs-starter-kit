---
uid: u6548263
name: Haoyang Chen
project_supervisor: Ben Swift
course_supervisor: Jochen Renz
course_title: Advanced Computing Research Project (COMP4550)
course_units: 12 + 12 
semester: 1 and 2
---

NOTE: the actual pdf form which needs to be submitted is
[here](http://courses.cecs.anu.edu.au/courses/CSPROJECTS/Independent_Study_Contract.pdf).
However, collaboratively editing a pdf is a bit gross, so you should write your
project up in this markdown file and then copy-paste it into the pdf form just
before submission.

## Section A (Students and Supervisors)

### Learning Objectives

1. Learn to train LSTM networks to generate music.

2. Understand how machines recognize feelings converyed by music.

3. Learn to design a good musical interface.

4. Learn to subjectively and objectively evaluate a musical system.

_(add/remove as required)_

### Project Description

The overall direction for this research project is to build a system to generate emotional music using machine learning techniques such as LSTM (long short term memory) networks and some general classification methods with chord progression pattern as input to learn. The corresponding model will be (hopefully) able to distinguish different chord patterns that brings people different feelings. The generated music with new chord progressions should be able to give audience similar moods/feeling as the input pattern will bring (if successful). 

The work for this research project could be divided into four parts: manually summarising some mainstream (and potentially side stream) chord patterns that brings people different feelings (emotions) upon listening, training LSTM models with MIDI files that contains particular chord progressions and classifying each generated new piece of music to the corresponding type based on feelings (emotions) of people, building a system that can interact with users, and finally evaluate the new piece of music both subjectively and objectively. Detailed plans for the steps are shown below: 

1. The first step is about data preparation. Before collecting the data, we should first manually summaries some general chord progressions that gives people certain emotions. Out of countless combinations of chords, many have already been widely adopted to produce music that satisfy different emotional requirements (e.g. Chord CDE) and we will take advantage of those. There are many MIDI files that contains various chord progressions (e.g. Magenta dataset). For research purposes, we need to manually select the files that contains certain emotional chord patterns. These files will then be converted to a format that LSTM network can understand for training purposes. 

2. The second step is about network training. Since the input format of LSTM should be based on sequence data, we should preserve some notes in the original file so that the generated music is a complete sequence of notes and chords. Upon successful training of the model, the next step would be to classify each new piece of music based on the emotions conveyed by the sound. The expectation is that the system should be able to recognize what type of feeling it is generating from the music. 

3. The third step is to build a system / design a musical interface that can interact with users. The system should include some demos of pre-trained emotional melodies for users to try. It should also be able to take specified emotions as inputs and generate music that convey corresponding feelings. Another module involved could be a self-tuned mixed emotion generator. This module can combine melodies with different conveyed feeling and output some new styles. 

3. The final step is to evaluate the performance of the system along with the model. This could be done subjectively and objectively. We will manually inspect the chord progressions generated by the model by playing it on a digital software or a guitar. The music will also be played to audiences from different backgrounds and a survey may be conducted to investigate what kind of feelings and emotions do they have after listening to the music. From the objective point of view, we will use the formal music metrics proposed in “On the evaluation of generative of models in music” to evaluate the performance. We will then compare the result with current state of the art benchmarks to gain a better understanding of the model performance. 

### Assessment

_See form_

### Meeting Dates

Weekly (group meeting) and fortnightly (individual meeting).
