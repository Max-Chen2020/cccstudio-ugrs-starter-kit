---
uid: u6548263
name: Haoyang Chen
project_supervisor: Ben Swift
course_supervisor: Jochen Renz
course_title: Advanced Computing Research Project (COMP4550)
course_units: 12 + 12 
semester: 1 and 2
---

NOTE: the actual pdf form which needs to be submitted is
[here](http://courses.cecs.anu.edu.au/courses/CSPROJECTS/Independent_Study_Contract.pdf).
However, collaboratively editing a pdf is a bit gross, so you should write your
project up in this markdown file and then copy-paste it into the pdf form just
before submission.

## Section A (Students and Supervisors)

### Learning Objectives

1. Learn to use neural network to generate music.

2. Understand the relationship between chord progressions and feelings / emotions arisen from listening.

3. Learn to subjectively and objectively evaluate a musical system / model.

_(add/remove as required)_

### Project Description

The overall direction for this research project is to examine the relationship between chord progressions and the emotions / feelings arisen from listening, and then to build a model that takes cps with labeled emotions and generates new cps which (hopefully) bring people new feelings.
The work for this project could be divided into four parts: gathering dataset, building relationship mapping model, training a ML model and evaluating the output. Detailed steps are shown below:
1. The first step requires identifying cps that can bring people different feelings. The chord progressions could be either mainstream or uncommon and may come from various types of music (e.g., pop, country). Once the chord progressions are identified and chosen, the next step would be to transform them into audio files that could be used as dataset later.
2. The second step is to build a relationship model that maps audio files with corresponding chord progressions to emotions / feelings. The system of mapping should be based on arousal / valence graph so that all kinds of feelings could be represented. A survey or performance may be carried out to gather people’s feelings upon listening to the audios.
3. The third step is to train a model. The model should take the identified audio files as input and generate some new patterns of music. The network that used in the model could be long-short term memory or transformers. Various other network models may be applied and compared to get a better result.
4. The final step is to evaluate the performance of the model. This could be done both subjectively and objectively. We will manually inspect the chord progressions generated by the model by playing it on a digital software or a guitar. The music will be played to audiences from different backgrounds and a survey may be conducted to investigate what kind of feelings and emotions do they have after listening to the music. From the objective point of view, we will use the formal music metrics proposed in “On the evaluation of generative of models in music” [1] and some common machine learning techniques to evaluate the performance. We will then compare the result with current state of the art benchmarks to gain a better understanding of the model performance.
Future work could be carried out on building a generative music model that outputs audio with specified feelings / emotions. Another potential research direction could be to mix different generated music to create new feelings / emotions.


### Assessment

_See form_

### Meeting Dates

Weekly (group meeting) and fortnightly (individual meeting).
